{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 Desicion Tree\n",
    "\n",
    "*   Training file - 'id3-train.dat'\n",
    "*   Testing file - 'id3-test.dat'\n",
    "\n",
    "In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.\n",
    "\n",
    "In a decision tree, if you reach a leaf node but still have examples that belong to different classes, then choose the most frequent class (among the instances at the leaf node). If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes, then choose the class that is most frequent in the entire training set. You do not need to implement pruning. Also, donâ€™t forget to use logarithm base 2 when computing entropy and set (0 log 0) to 0.\n",
    "\n",
    "Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments. The code should output the following 3 things:\n",
    "\n",
    "*   Print the Decision Tree created, in the following example format:\n",
    "\n",
    "    ```\n",
    "    attr1 = 0 :\n",
    "        attr2 = 0 :\n",
    "            attr3 = 0 : 1 -- 4\n",
    "            attr3 = 1 : 0 -- 9\n",
    "        attr2 = 1 :\n",
    "            attr4 = 0 : 0 -- 2\n",
    "            attr4 = 1 : 1 -- 10\n",
    "    attr1 = 1 :\n",
    "        attr2 = 1 : 1 -- 17\n",
    "\n",
    "    ```\n",
    "\n",
    "*   Accuracy on the Training data = x %\n",
    "*   Accuracy on the Test data = x %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the data files are in the following folder -- \n",
    "basePath = \"/Users/martgom/Documents/UTSA/FALL-2023/Artificial Intelligence/HW3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file name variables\n",
    "train = basePath + \"/id3-train.dat\"\n",
    "test = basePath + \"/id3-test.dat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NODE\n",
    "class Node:\n",
    "    def __init__(self, attribute = None, value = None, leaf_class = None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.leaf_class = leaf_class\n",
    "        self.children = {}\n",
    "        \n",
    "#calculate the disorder of dataset\n",
    "def calculate_e(data):\n",
    "    total_ins = len(data)\n",
    "    class_lbl = [instance[-1] for instance in data]\n",
    "    class_c = {}\n",
    "    \n",
    "    for lbl in class_lbl:\n",
    "        if lbl in class_c:\n",
    "            class_c[lbl]+=1\n",
    "        else:\n",
    "            class_c[lbl] = 1\n",
    "    disorder = 0\n",
    "    for count in class_c.values():\n",
    "        prob = count / total_ins\n",
    "        disorder = disorder - prob * math.log2(prob)\n",
    "    return disorder\n",
    "\n",
    "#calculate information \n",
    "def calculate_info_gain(data, att_index):\n",
    "    total_e = calculate_e(data)\n",
    "    att_values = [0,1]\n",
    "    weight_e = 0\n",
    "    \n",
    "    for value in att_values:\n",
    "        sub = [instance for instance in data if instance[att_index] == value]\n",
    "        sub_e = calculate_e(sub)\n",
    "        sub_weight = len(sub)/len(data)\n",
    "        weight_e += sub_weight *sub_e\n",
    "    return total_e - weight_e\n",
    "\n",
    "# Pseudocode for the ID3 algorithm. Use this to create function(s).\n",
    "def ID3(data, root, attributesRemaining):\n",
    "    # If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes\n",
    "    if not data:\n",
    "        # Choose and the class that is most frequent in the entire training set and return the updated tree\n",
    "        return root\n",
    "    # If all the instances have only one class label\n",
    "    class_lbl = [instance[-1] for instance in data]\n",
    "    if len(set(class_lbl)) == 1:\n",
    "        # Make this as the leaf node and use the label as the class value of the node and return the updated tree\n",
    "        root.leaf_class = class_lbl[0]\n",
    "        return root\n",
    "    # If you reached a leaf node but still have examples that belong to different classes (there are no remaining attributes to be split)\n",
    "    if not attributesRemaining:\n",
    "        # Assign the most frequent class among the instances at the leaf node and return the updated tree\n",
    "        root.leaf_class = max(set(class_lbl), key=class_lbl.count)\n",
    "        return root\n",
    "    # Find the best attribute to split by calculating the maximum information gain from the attributes remaining by calculating the entropy\n",
    "    best_at_index , best_info_gain = -1, -1\n",
    "    \n",
    "    for attribute_index in attributesRemaining:\n",
    "        info_gain = calculate_info_gain(data, attribute_index)\n",
    "        if info_gain > best_info_gain:\n",
    "            best_at_index = attribute_index\n",
    "            best_info_gain = info_gain\n",
    "        \n",
    "        if best_at_index == -1:\n",
    "            root.leaf_class = max(set(class_lbl), key = class_lbl.count)\n",
    "            return root\n",
    "        best_at = best_at_index\n",
    "        root.attribute = best_at\n",
    "    # Split the tree using the best attribute and recursively call the ID3 function using DFS to fill the sub-tree\n",
    "    for attribute_value in [0 , 1]:\n",
    "        sub = [instance for instance in data if instance[best_at]==attribute_value]\n",
    "        if not sub:\n",
    "            leaf_class = max(set(class_lbl), key = class_lbl.count)\n",
    "            root.children[attribute_value] = Node(leaf_class = leaf_class)\n",
    "        else:\n",
    "            root.children[attribute_value] = ID3(sub, Node(), [attribute for attribute in attributesRemaining if attribute !=best_at])\n",
    "            \n",
    "    # return the root as the tree\n",
    "    return root\n",
    "\n",
    "    #print desicion tree\n",
    "def print_decision(node, level = 0):\n",
    "    if node.leaf_class is not None:\n",
    "        print(f'{node.leaf_class} --- {len(content)}')\n",
    "    else:\n",
    "        for value, child_nd in node.children.items():\n",
    "            print('\\t' * level + f'attribute-{node.attribute} = {value}:', end=' ')\n",
    "            print_decision(child_nd, level +1)\n",
    "\n",
    "#read file\n",
    "def read_data_file(filename) :\n",
    "    with open(filename, 'r') as file:\n",
    "        #skip first line\n",
    "        next(file)\n",
    "        content = [list(map(int, line.strip().split())) for line in file.readlines()]\n",
    "    return content\n",
    "#read content callinf read_data_file method\n",
    "content = read_data_file(train)\n",
    "#get number of attributes\n",
    "num_att = len(content[0])-1\n",
    "#list of attributes\n",
    "train_att = list(range(num_att))\n",
    "\n",
    "#create the desicion tree\n",
    "root = Node()\n",
    "dec_tree = ID3(content, root, train_att)\n",
    "\n",
    "#print\n",
    "print_decision(dec_tree)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is the base code structure. Feel free to change the code structure as you see fit, maybe even create more functions.\n",
    "#predict labels of the tree\n",
    "def predict_lbl(tree, attributes):\n",
    "    if tree.leaf_class is not None:\n",
    "        return tree.leaf_class\n",
    "    \n",
    "    attribute_value = attributes[tree.attribute]\n",
    "    \n",
    "    if attribute_value in tree.children:\n",
    "        return predict_lbl(tree.children[attribute_value], attributes)\n",
    "    else:\n",
    "        \n",
    "        return tree.leaf_class\n",
    "    \n",
    "        \n",
    "# Read the first line in the training data file, to get the number of attributes\n",
    "\n",
    "# Read all the training instances and the ground truth class labels.\n",
    "# Create the decision tree by implementing the ID3 algorithm. Pseudocode provided above.\n",
    "def print_desicion_tree(node, att_names, level = 0):\n",
    "    if node.leaf_class is not None:\n",
    "        return\n",
    "    \n",
    "    for value, child_nd in node.children.items():\n",
    "        prefix = \" \" * level\n",
    "        att_nm = att_names[node.attribute]\n",
    "        data_len = len(child_nd.data) if hasattr(child_nd, \"data\") else 0\n",
    "        print(f\"{prefix}{att_nm} = {value} : {data_len} -- {child_nd.leaf_class}\")\n",
    "        print_desicion_tree(child_nd, att_names, level+1)\n",
    "    \n",
    "#calculate accuracy\n",
    "def cal_accuracy( content, tree):\n",
    "    \n",
    "    predictions = 0\n",
    "    total = len(content)\n",
    "    # For each training instance, predict the output label\n",
    "    for instance in content:\n",
    "        #do not include last column and get label from the current database\n",
    "        att= instance[:-1]\n",
    "        current_class = instance[-1]\n",
    "        pred_class = predict_lbl(tree, att)\n",
    "        \n",
    "        # Compare it with the ground truth class label and calculate the accuracy accordingly\n",
    "        #check using the tree\n",
    "        if pred_class == current_class:\n",
    "            \n",
    "            predictions +=1\n",
    "        #calculate and return accuracy\n",
    "        #print(predictions)\n",
    "        #print(total)\n",
    "    accuracy = ((predictions / total) * 100)\n",
    "    return accuracy\n",
    "    \n",
    "            \n",
    "        \n",
    "#main method \n",
    "def main():\n",
    "    content = read_data_file(train)\n",
    "    #print(content)\n",
    "    num_att = len(content[0] )-1\n",
    "    train_att = list(range(num_att))\n",
    "    \n",
    "    #create the tree\n",
    "    root = Node()\n",
    "    dt = ID3(content,root, train_att)\n",
    "    #create attributes defined\n",
    "    attributes = []\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        attribute = f\"attribute-{i}\"\n",
    "        attributes.append(attribute)\n",
    "    attributes.append(\"Class\")\n",
    "    #print tree\n",
    "    print_desicion_tree(dt, attributes)\n",
    "    \n",
    "    train_acc = cal_accuracy(content, dt)\n",
    "    if train_acc is not None:\n",
    "        print(f\"Accuracy on Train: = {train_acc:.2f}%\")\n",
    "    else:\n",
    "        print(\"ERROR Train.\")\n",
    "    \n",
    "    data_test = read_data_file(test)\n",
    "    #print accuracy of the training data\n",
    "    test_acc = cal_accuracy(data_test, dt)\n",
    "    if test_acc is not None:\n",
    "        print(f\"Accuracy on Test: = {test_acc:.2f}%\")\n",
    "    else:\n",
    "        print(\"ERROR.\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve representation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through to select the number of instances 'x' in increments of 40\n",
    "def generate_curve(train_data, test_data, step = 40):\n",
    "    n_instances = len(train_data)\n",
    "    n_att = len(train_data[0])-1\n",
    "    train_att = list(range(n_att))\n",
    "    \n",
    "    xv =[]\n",
    "    yv = []\n",
    "    # For each 'x',\n",
    "    for x in range(step, n_instances +1 ,step):\n",
    "        # Randomly select 'x' instances\n",
    "        random_data = random.sample(train_data, x)\n",
    "    \n",
    "        # Create the ID3 decision tree using those instances\n",
    "        dt = ID3(random_data, root, train_att)\n",
    "        # Calculate the accuracy of the ID3 tree created on the Test data\n",
    "        accuracy = cal_accuracy(test_data, dt)\n",
    "        \n",
    "        xv.append(x)\n",
    "        yv.append(accuracy)\n",
    "    return xv,yv\n",
    "\n",
    "test_data = read_data_file(test)\n",
    "xv, yv = generate_curve(content, test_data)        \n",
    "# Plot the learning curve using the accuracy values\n",
    "plt.plot(xv, yv, marker = '.')\n",
    "plt.title(\"Learning Curve Graph\")\n",
    "\n",
    "# X-axis will be the number of training instances used for creating the tree\n",
    "plt.xlabel(\"Number of instances\")\n",
    "# Y-axis will be the accuracy in % on the Test data\n",
    "plt.ylabel(\"Test-Acurracy Percentage\")\n",
    "plt.grid(True), plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
